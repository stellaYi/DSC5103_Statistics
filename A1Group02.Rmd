---
title: "DSC5103 Assignment 1"
subtitle: "Simulation and K-Nearest Neighbor Algorithm"
author: "Tong Wang"
date: "Aug 2018"
output:
  html_document:
    theme: yeti
    highlight: tango
  pdf_document:
    highlight: zenburn
---
<!--
comments must be put in an HTML comment form
-->

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)  # set output width
```

## NOTE:
This assignment is **due at 23:59 of Aug 30, Thursday**. You can work on this file directly and fill in your answers/code below. Please submit the output HTML file (name your file like A1Group02.html if you are from Group 02 of Section A1) onto IVLE/Files/Student Submission/Assignment1 folder.

Also, put the Section/Group and member info below.
```{r}
# Section A1
# Group 02
# Members: 
# Wai Kin Simon, Chee - A0186100U
# Yi Suqin (Stella)   - A0026606L
# Zhang Kang En       - A0186050L
# Sin Yu Fan          - A0139284X

```



## Part I: Simulation
In this exercise, we will use simulation to illustrate the variability of statistics calculated from random samples. Suppose there is a **normal** population of size **10000**, with mean **100** and standard deviation **15**. Now we draw a sample from the population, of size **100** without replacement, we can calculate sample statistics such as mean and variance. If we further repeat the sampling process many times, say **200**, we will have 200 sets of similar sample statistics. Let's examine these sample statistics.

The necessary parameters are already set up as below.
```{r}
pop.size <- 10000
pop.mean <- 100
pop.sd <- 15

num.of.samples <- 200
sample.size <- 100
```

### Questions
#### 1. Use random seed **1234** to conduct the simulation (i.e., simulate the population as specified, draw 200 samples, and calculate sample mean and variance for each sample, respectively), evaluate the mean and standard deviation of the sample statistics, and compare with their theoretical values. Draw histograms of the sample statistics. (1 Mark)

Answer: 

```{r}

library("ggplot2")

# put your R code here inside the blocks
set.seed(1234)
simulation <- rnorm(n=pop.size, mean=pop.mean, sd=pop.sd)

# Draw 200 samples of 100 from population without replacement

mean.list <- numeric(length=num.of.samples)
sd.list  <- numeric(length=num.of.samples)

for (i in 1:num.of.samples) {
    sampleData <-  sample(simulation, sample.size, replace=FALSE)
    mean.list[i] <- mean(sampleData)   
    sd.list[i] <- sd(sampleData)
}

# Calculate final statistics (mean and SD)
mean200 <- mean(mean.list) 
sd200 <- sd(mean.list)

sprintf("Total Mean: %s",  mean200)
sprintf("Total SD: %s", sd200)

# Histogram of means of 200 samples
plotMean <-ggplot(data=data.frame(mean.list), mapping = aes(x=mean.list)) + geom_histogram(binwidth = 0.3, aes(y=..density..)) + labs(title="Histogram for means for 200 samples") + geom_vline(aes(xintercept=mean(mean200)),color="blue", linetype="dashed", size=1) + geom_density()
plotMean

# Histogram of sd of 200 samples
plotSd <-ggplot(data= data.frame(sd.list), mapping = aes(x=sd.list)) + geom_histogram(binwidth = 0.3, aes(y=..density..)) + labs(title="Histogram for sd for 200 samples") + geom_vline(aes(xintercept=mean(sd.list)),color="blue", linetype="dashed", size=1) + geom_density()
plotSd

```



## Part II: K-Nearest Neighbor Algorithm

### Introduction
In this assignment, we are going to experiment the K-Nearest Neighbor (KNN) algorithm on a higher-dimensional dataset and experience the deterioration of prediction performance as the dimensionality grows.

The experiment is built on top of the 3rd-order polynomial model discussed in class (knn_demo.R), i.e.,
$$y = \beta_0 + \beta_1 * x + \beta_2 * x^2 + \beta_3 * x^3 + \epsilon, ~~ \epsilon \sim \text{N}(0, \sigma^2)$$
and we are going to introduce an extra 20-dimensional predictor $z$, which does NOT actually play a role in generating $y$. Yet, when in estimation, we do not know the fact and will use both $x$ and $z$ as predictors in the KNN algorithm.

### Generation of the high-dimensional dataset
We first simulate the 3rd-order polynomial datasets as we did in knn_demo.R. 

```{r}

## population parameters
beta0 <- 1
beta1 <- -2
beta2 <- 6
beta3 <- -1
sigma <- 2

set.seed(7890)

## training data
x <- runif(n=100, min=0, max=5) 
f_x <- beta0 + beta1 * x + beta2 * x^2 + beta3 * x^3
epsilon <- rnorm(n=length(x), mean=0, sd=sigma)
y <- f_x + epsilon

## test data
x.test <- runif(n=50, min=0, max=5)
f_x.test <- beta0 + beta1 * x.test + beta2 * x.test^2 + beta3 * x.test^3
epsilon.test <- rnorm(n=length(x.test), mean=0, sd=sigma)
y.test <- f_x.test + epsilon.test
```
The resulted training and test dataset have `r length(y)` and `r length(y.test)` data points, respectively.

Next, we need to generate $z$, the 20-dimensional predictors, of the same sizes. Each $z$ is a 20-dimensional multivariate normal random variable, with mean being $(0, 0, \ldots, 0)$ and identity covariance matrix (so that the 20 elements are independent standard normal random variables). The resulted $z$ is a 100*20 matrix, with each row being a data point with 20 dimensions.
```{r}
library("mvtnorm")  # package for multivariate normal distribution, INSTALL IT BEFORE RUNNING
z <- rmvnorm(n=length(x), mean=rep(0, 20))  # covariance matrix is identity matrix by default, no need to specify here
z.test <- rmvnorm(n=length(x.test), mean=rep(0, 20))
head(z)
```

Later, we will use $(x, z)$ to predict $y$. Let's first combine $x$ and $z$ into matrices, as required by function knn.reg().
```{r}
train.x <- cbind(x, z)
test.x <- cbind(x.test, z.test)
head(train.x)
```

### Questions

#### 1.	For a fixed $k=15$, fit a KNN model to predict $y$ with $(x, z)$, and measure the training and test MSE. (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks
# install.packages("FNN")
library("FNN")
?knn.reg

# Train model
model15.train <- knn.reg(train = train.x, test = train.x, y = y, k = 15)

# Calculate training MSE
model15.train.mse <- mean((y - model15.train$pred)^2)

# Test model
model15.test <- knn.reg(train = train.x, test = test.x, y = y, k = 15)

# Calculate testing MSE
model15.test.mse <- mean((y.test - model15.test$pred)^2)

sprintf("Training MSE: %s",  model15.train.mse)
sprintf("Testing MSE: %s",  model15.test.mse)

```


#### 2.	With the same data, plot the training and test MSE of the KNN model against $k$, and find the optimal $k$ and the corresponding test MSE.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks

## Training and Test Error plot: to enumerate many many k values and measure MSE
# k's that will be evaluated
ks <- 1:30

# construct empty vectors for keeping the MSE for each k
mse.train <- numeric(length=length(ks))
mse.test  <- numeric(length=length(ks))

# loop over all the k and evaluate MSE in each of them
for (i in seq(along=ks)) {
    model.train <- knn.reg(train.x, train.x, y, k=ks[i])
    model.test  <- knn.reg(train.x, test.x, y, k=ks[i])
    mse.train[i] <- mean((y - model.train$pred)^2)
    mse.test[i] <- mean((y.test - model.test$pred)^2)
}

# optimal k
k.opt <- ks[which.min(mse.test)]
# optimal MSE
mse.opt <- min(mse.test)

sprintf("Optimal K: %s",  k.opt)
sprintf("Optimal MSE: %s",  mse.opt)

# plot
ggplot() + geom_line(aes(x=ks, y=mse.train), color="red") + geom_point(aes(x=ks, y=mse.train)) + geom_line(aes(x=ks, y=mse.test), color="blue") + geom_point(aes(x=ks, y=mse.test)) + scale_x_reverse(lim=c(30, 1)) + geom_hline(yintercept=sigma^2, linetype=2) + theme_bw()

```

#### 3.	Based on the analysis above, compare the above model with $(x, z)$ being the predictors and the previous model with $x$ only (as in knn_demo.R). Briefly explain why.  (1 Mark)

Answer: 


With higher dimensionality, the optimal $k$ value increased from 3 to 7 and resulted in worsen MSE increasing from 4.804397 to 26.56613. This is likely the result of curse of dimensionality. The additional dimension (z) in this causes sparseness in the training data, thus causing the optimal K value to increase and give rise to less accurate prediction of Y.


#### 4.	We have seen that the test MSE is significantly worse than what we had without using predictor $z$ (in knn_demo.R). To better understand the impact of including irrelevant predictors in the KNN algorithm, let's try to include the 20 dimensions of $z$ one by one. So in each round $j$, we construct the predictors by combining $x$ and the first $j$ columns of $z$, then repeat the analysis in Question 2 and find the optimal $k$ and test MSE. At the end, plot the optimal MSE agaist $j$, and interpret the result.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks

# define the # of additional dimension (js) and # of k
js <- 1:21
ks <- 1:30

# construct empty vectors for keeping the optimum K and MSE for each j iteration
MSE.opt.all <- numeric(length = length(js))
k.opt.all <- numeric(length = length(js))

for (i in seq(along=js)) {
  
  # without z
  if (i==1) {
    train.x <- matrix(x, ncol = 1)
    test.x <- matrix(x.test, ncol = 1)
  }
  
  if (i>1) {
    train.x <- cbind(x, z[, 1:i-1])
    test.x <- cbind(x.test, z.test[, 1:i-1])
  }
  
  
  MSE.test <- numeric(length = length(ks))
  
  for (j in seq(along=ks)) {
    model.test <- knn.reg(train = train.x, test = test.x, y = y, k = j)
    mse.test[j] <- mean((y.test - model.test$pred)^2)
    
  }
  
  # Optimal k
  k.opt.all[i] <- ks[which.min(mse.test)]
  
  # Optimal MSE
  MSE.opt.all[i] <- min(mse.test)
  
}

#the optimum k over j iterations
k.opt.all[which.min(MSE.opt.all)]

#the minimum test mean square error
min(MSE.opt.all)

#plot
ggplot() +
  geom_line(aes(x = js-1, y = MSE.opt.all, color = "test")) +
  geom_point(aes(x = js-1, y = MSE.opt.all, color = "test")) +
  scale_color_manual("", values = c("test" = "red")) +
  ylab("MSE") +
  xlab("No. of z's dimension included") +
  geom_hline(yintercept = sigma^2, linetype = 2) +
  theme_bw()

```



## Session Info

```{r session-info}
print(sessionInfo(), locale=FALSE)
```
